import os
import sys
from typing import Optional
import logging
import json
from datetime import datetime, timedelta

# Disable LangChain debug logs
logging.getLogger("langchain").setLevel(logging.ERROR)

# Database and LangChain imports
from langchain_community.utilities import SQLDatabase
from langchain_community.tools import QuerySQLDatabaseTool
from langchain_huggingface import HuggingFacePipeline
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from sqlalchemy import create_engine, text
import langchain
langchain.debug = False  # Disable debug logging

# Model imports
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Configuration
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
DB_CONFIG = {
    "user": "root",
    "password": "admin",
    "host": "localhost",
    "port": 3306,
    "database": "chatbot"
}

def load_tinyllama_model():
    """Load the TinyLlama model for text generation."""
    print("‚è≥ Loading TinyLlama model...")
    
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # Check for GPU availability
    if torch.cuda.is_available():
        print("üöÄ Using GPU for inference")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            device_map="auto"
        )
    else:
        print("‚öôÔ∏è Using CPU for inference")
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        
    print("‚úÖ Model loaded!")
    
    # Create text generation pipeline with proper settings
    hf_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        do_sample=True,  # Enable sampling for temperature and top_p to work
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
    )
    
    # Wrap in LangChain format
    return HuggingFacePipeline(pipeline=hf_pipe)

def get_db_connection():
    """Create a database connection string and SQLDatabase object."""
    db_uri = f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
    return db_uri

def select_relevant_tables(question, all_tables):
    """Select tables that might be relevant to the question."""
    question_lower = question.lower()
    
    # First try exact matches
    relevant = [t for t in all_tables if t.lower() in question_lower]
    
    # If no matches, use keyword mapping
    if not relevant:
        keywords = {
            "order": ["orders", "order_details"],
            "product": ["products", "inventory_transactions"],
            "customer": ["customers"],
            "inventory": ["inventory_transactions", "products"],
            "purchase": ["orders", "order_details"],
            "buy": ["orders", "order_details"],
            "ship": ["orders"],
            "deliver": ["orders"],
            "sale": ["orders", "order_details"],
            "payment": ["orders"],
            "stock": ["inventory_transactions", "products"]
        }
        
        for keyword, tables in keywords.items():
            if keyword in question_lower:
                for table in tables:
                    if table in all_tables and table not in relevant:
                        relevant.append(table)
    
    # Fallback if still no matches
    return relevant if relevant else all_tables[:5]  # Limit to first 5 to avoid overwhelming
def create_sql_generation_chain(llm, db):
    """Create a chain that generates SQL from natural language."""
    # Create a more explicit prompt template
    sql_prompt = PromptTemplate(
        input_variables=["input", "table_info", "dialect", "top_k"],
        template="""You are an expert SQL query writer. Your task is to convert natural language questions into SQL queries.

Database Schema Information:
{table_info}

Database dialect: {dialect}

User Question: {input}

Top {top_k} most similar tables:
{table_info}

Write a SQL query to answer the user's question.
SQL Query:"""
    )
    
    # Create the chain
    chain = create_sql_query_chain(
        llm=llm,
        db=db,
        prompt=sql_prompt
    )
    
    return chain

def execute_sql_query(query, db_uri):
    """Execute a SQL query and return the results."""
    try:
        # Create engine
        engine = create_engine(db_uri)
        
        # Execute query
        with engine.connect() as conn:
            result = conn.execute(text(query))
            columns = result.keys()
            rows = result.fetchall()
            
        # Format results
        formatted_results = []
        for row in rows:
            formatted_results.append(dict(zip(columns, row)))
            
        return {
            "success": True,
            "query": query,
            "results": formatted_results,
            "column_names": columns
        }
    except Exception as e:
        return {
            "success": False,
            "query": query,
            "error": str(e)
        }

def format_results(query_results):
    """Format the results in a readable way."""
    if not query_results["success"]:
        return f"‚ùå Error executing query: {query_results['error']}"
    
    if not query_results["results"]:
        return "üì≠ No results found for this query."
    
    # Format as a table
    result_str = "\nüìä Query Results:\n"
    
    # Get column widths
    sample_row = query_results["results"][0]
    col_widths = {col: max(len(str(col)), max(len(str(row[col])) for row in query_results["results"])) 
                 for col in sample_row.keys()}
    
    # Header
    header = " | ".join(f"{col:{col_widths[col]}}" for col in sample_row.keys())
    separator = "-" * len(header)
    
    result_str += f"{header}\n{separator}\n"
    
    # Rows
    for row in query_results["results"]:
        result_str += " | ".join(f"{str(val):{col_widths[k]}}" for k, val in row.items()) + "\n"
    
    return result_str

def handle_time_expressions(query):
    """Handle common time expressions in the query."""
    today = datetime.now()
    replacements = {
        "last week": f"BETWEEN '{(today - timedelta(days=7)).strftime('%Y-%m-%d')}' AND '{today.strftime('%Y-%m-%d')}'",
        "this week": f"BETWEEN '{(today - timedelta(days=today.weekday())).strftime('%Y-%m-%d')}' AND '{today.strftime('%Y-%m-%d')}'",
        "last month": f"BETWEEN '{(today.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')}' AND '{(today.replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')}'",
        "this month": f"BETWEEN '{today.replace(day=1).strftime('%Y-%m-%d')}' AND '{today.strftime('%Y-%m-%d')}'",
        "yesterday": f"= '{(today - timedelta(days=1)).strftime('%Y-%m-%d')}'",
        "today": f"= '{today.strftime('%Y-%m-%d')}'"
    }
    
    # Find if query contains time expressions
    for expr, replacement in replacements.items():
        if expr in query.lower():
            query = query.replace(expr, f"date_expression_{expr}")
    
    return query

def main():
    # Load the model
    llm = load_tinyllama_model()
    
    # Connect to the database
    db_uri = get_db_connection()
    
    # Create database explorer
    print("üìä Connecting to database...")
    db = SQLDatabase.from_uri(db_uri)
    all_tables = db.get_usable_table_names()
    print(f"‚úÖ Connected! Available tables: {', '.join(all_tables)}")
    
    print("\nüîπ Natural Language to SQL Chatbot\n")
    print("Type 'exit' or 'quit' to stop.")
    print("Type 'tables' to see available tables.")
    print("Type 'schema <table>' to see table structure.\n")
    
    while True:
        # Get user input
        user_question = input("\nUser Question: ").strip()
        
        # Check for exit commands
        if user_question.lower() in ["exit", "quit"]:
            print("üëã Goodbye!")
            break
            
        # Check for special commands
        if user_question.lower() == "tables":
            print(f"Available tables: {', '.join(all_tables)}")
            continue
            
        if user_question.lower().startswith("schema "):
            table_name = user_question[7:].strip()
            if table_name in all_tables:
                engine = create_engine(db_uri)
                with engine.connect() as conn:
                    result = conn.execute(text(f"DESCRIBE {table_name}"))
                    rows = result.fetchall()
                print(f"\nSchema for {table_name}:")
                for row in rows:
                    print(f"  {row[0]}: {row[1]}")
            else:
                print(f"Table '{table_name}' not found.")
            continue
            
        if not user_question:
            continue
            
        try:
            # Process the time expressions in the question
            processed_question = handle_time_expressions(user_question)
            
            # Select relevant tables
            relevant_tables = select_relevant_tables(processed_question, all_tables)
            print(f"üîç Using tables: {', '.join(relevant_tables)}")
            
            # Create a database connection with only relevant tables
            filtered_db = SQLDatabase.from_uri(db_uri, include_tables=relevant_tables)
            
            # Create the SQL generation chain
            chain = create_sql_generation_chain(llm, filtered_db)
            
            # Generate SQL
            print("‚è≥ Generating SQL query...")
            sql_query = chain.invoke({"question": processed_question, "dialect": "mysql"})
            
            # Extract just the SQL (model might add extra text)
            sql_lines = []
            for line in sql_query.split('\n'):
                if line.strip() and not line.strip().startswith('--') and not line.lower().startswith('sql'):
                    sql_lines.append(line)
            final_sql = '\n'.join(sql_lines).strip()
            
            # Print the generated SQL
            print(f"\nüìù Generated SQL Query:\n{final_sql}\n")
            
            # Execute the query
            print("‚è≥ Executing query...")
            results = execute_sql_query(final_sql, db_uri)
            
            # Format and display results
            print(format_results(results))
            
        except Exception as e:
            print(f"\n‚ùå Error: {str(e)}")
            import traceback
            print(traceback.format_exc())

if __name__ == "__main__":
    main()




































































































































































































































































































































































































































!pip install torch transformers bitsandbytes accelerate sqlparse

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

torch.cuda.is_available()

available_memory = torch.cuda.get_device_properties(0).total_memory
print(available_memory)

model_name = "defog/sqlcoder-7b-2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
if available_memory > 15e9:
    # if you have atleast 15GB of GPU memory, run load the model in float16
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",
        use_cache=True,
    )
else:
    # else, load in 8 bits ‚Äì¬†this is a bit slower
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        # torch_dtype=torch.float16,
        load_in_8bit=True,
        device_map="auto",
        use_cache=True,
    )

import sqlparse

def generate_query(question):
    updated_prompt = prompt.format(question=question)
    inputs = tokenizer(updated_prompt, return_tensors="pt").to("cuda")
    generated_ids = model.generate(
        **inputs,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens=400,
        do_sample=False,
        num_beams=1,
    )
    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    # empty cache so that you do generate more results w/o memory crashing
    # particularly important on Colab ‚Äì memory management is much more straightforward
    # when running on an inference service
    return sqlparse.format(outputs[0].split("[SQL]")[-1], reindent=True)




















































































































































PS C:\Users\AD54619\Text2Sql\Sqlcoder> python search_schema.py

üîç Enter your natural-language question (type 'exit' to quit):

‚ùì Your question: which table stores information  about customers
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\Sqlcoder\search_schema.py", line 55, in <module>
    interactive_loop()
    ~~~~~~~~~~~~~~~~^^
  File "C:\Users\AD54619\Text2Sql\Sqlcoder\search_schema.py", line 45, in interactive_loop
    results = get_top_k_tables(query, index, metadata, embed_model, top_k=3)
  File "C:\Users\AD54619\Text2Sql\Sqlcoder\search_schema.py", line 16, in get_top_k_tables
    D, I = index.search([query_embedding], top_k)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\faiss\class_wrappers.py", line 327, in replacement_search
    n, d = x.shape
           ^^^^^^^
AttributeError: 'list' object has no attribute 'shape'











































































No SQL generated
Error: (pymysql.err.OperationalError) (1065, 'Query was empty') (Background on this error at: https://sqlalche.me/e/20/e3q8)


from llama_cpp import Llama

def main():
    # Hardcoded model path and parameters
    MODEL_PATH = "sqlcoder-7b-2.Q4_K_M.gguf"  # REPLACE THIS WITH YOUR ACTUAL MODEL PATH
    N_CTX = 2048
    N_THREADS = 6
    VERBOSE = True
    
    # Initialize the model with optimal settings for performance
    print(f"Loading model from {MODEL_PATH}...")
    model = Llama(
        model_path=MODEL_PATH,
        n_gpu_layers=0,      # CPU only
        n_ctx=N_CTX,         # Context size
        n_threads=N_THREADS, # CPU threads
        verbose=VERBOSE,
        n_batch=512,         # Batch size for more efficient processing
        use_mlock=True,      # Lock memory to prevent swapping
        use_mmap=True,       # Use memory mapping for faster loading
        logits_all=False     # Only compute logits for the last token
    )
    print("Model loaded successfully!")

    # Sample table schema
    table_schema = """



    # SQLCoder prompt template
    prompt_template = """### Task
Generate a SQL query to answer the following question:
{question}

### Database Schema
The query will run on a database with the following schema:
{schema}

### SQL Query
```sql
"""

    while True:
        try:
            # Get user question
            user_question = input("\nEnter your question (or 'exit' to quit): ")
            if user_question.lower() in ['exit', 'quit']:
                break

            # Format the prompt
            prompt = prompt_template.format(
                question=user_question,
                schema=table_schema
            )

            # Generate SQL query with streaming for better UX
            print("\nGenerating SQL query...")
            
            print("\n--- Generated SQL Query ---")
            
            # Stream the response for better UX and automatic KV caching
            response = ""
            for chunk in model.create_completion(
                prompt,
                max_tokens=512,
                stop=["```"],
                echo=False,
                temperature=0.1,
                stream=True
            ):
                piece = chunk["choices"][0]["text"]
                response += piece
                print(piece, end="", flush=True)
            
            print("\n---------------------------")

        except KeyboardInterrupt:
            print("\nExiting...")
            break
        except Exception as e:
            print(f"Error: {str(e)}")

if __name__ == "__main__":
    main()






































































































































































































Available chat formats from metadata: chat_template.default
Guessed chat format: mistral-instruct
‚úÖ Model loaded!
‚è≥ Loading model from: sqlcoder-7b-2.Q4_K_M.gguf
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (16384) -- the full capacity of the model will not be utilized
‚úÖ Model loaded!
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\Webapp\backend\api.py", line 42, in <module>
    app.run(host="0.0.0.0", port=5000, debug=True)
    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\flask\app.py", line 657, in run
    cli.show_server_banner(self.debug, self.name)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\flask\cli.py", line 780, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\click\utils.py", line 318, in echo
    file.write(out)  # type: ignore
    ~~~~~~~~~~^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\click\_compat.py", line 542, in _safe_write
    return _write(s)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\colorama\ansitowin32.py", line 47, in write
    self.__convertor.write(text)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\colorama\ansitowin32.py", line 177, in write
    self.write_and_convert(text)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\colorama\ansitowin32.py", line 205, in write_and_convert
    self.write_plain_text(text, cursor, len(text))
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\colorama\ansitowin32.py", line 210, in write_plain_text
    self.wrapped.write(text[start:end])
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\click\_winconsole.py", line 192, in write
    return self._text_stream.write(x)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\click\_winconsole.py", line 177, in write
    raise OSError(self._get_error_message(GetLastError()))
OSError: Windows error 6
Exception ignored in: <function Llama.__del__ at 0x0000021D76598680>
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\llama.py", line 2205, in __del__
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\llama.py", line 2202, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 627, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 619, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 604, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 364, in __exit__
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\_internals.py", line 75, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 627, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 619, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 604, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 482, in _exit_wrapper
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\_internals.py", line 69, in free_model
TypeError: 'NoneType' object is not callable
Exception ignored in: <function Llama.__del__ at 0x0000021D76598680>
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\llama.py", line 2205, in __del__
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\llama.py", line 2202, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 627, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 619, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 604, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 364, in __exit__
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\_internals.py", line 75, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 627, in close
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 619, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 604, in __exit__
  File "C:\Users\AD54619\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 482, in _exit_wrapper
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\llama_cpp\_internals.py", line 69, in free_model
TypeError: 'NoneType' object is not callable
PS C:\Users\AD54619\Text2Sql\Webapp\backend>












































































































No SQL generated
Error: (pymysql.err.OperationalError) (1065, 'Query was empty') (Background on this error at: https://sqlalche.me/e/20/e3q8)








import os
import sys
from typing import Optional
import logging
import re

# Disable LangChain debug logs
logging.getLogger("langchain").setLevel(logging.ERROR)
import langchain
langchain.debug = False

# Import LangChain Community utilities and (we no longer use HuggingFacePipeline)
from langchain_community.utilities import SQLDatabase

from sqlalchemy import create_engine, inspect, text

import torch
# We no longer use transformers for model loading

# ----- New: Use Mistral-7B-Instruct-v0-2.Q4_km.gguf via llama-cpp-python -----
from llama_cpp import Llama

# Define a wrapper so our LLM interface remains the same:
class MistralLLM:
    def __init__(self, model_path: str, n_ctx: int, n_threads: int, verbose: bool):
        print("‚è≥ Loading Mistral-7B-Instruct model (llama-cpp)...")
        self.llama = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            verbose=verbose
        )
        print("‚úÖ Model loaded!")
    
    def __call__(self, prompt: str) -> str:
        # Call the model and return the generated text.
        output = self.llama(
            prompt,
            max_tokens=512,
            stop=["</s>","SQL:"])
        # Expecting output in the form: {"choices": [{"text": "..."}], ...}
        return output["choices"][0]["text"].strip()

# Set your model path and parameters here:
SQLCODER_MODEL_PATH = "sqlcoder-7b-2.Q4_K_M.gguf"  # replace with your actual file name

def load_sqlcoder_llm():
    return MistralLLM(
        model_path=SQLCODER_MODEL_PATH,  # same wrapper class
        n_ctx=2048,                      # or 1024 if you want smaller context
        n_threads=6,                     # match your CPU cores
        verbose=False
    )

def pick_tables(question: str, all_tables: list, llm) -> list:
    """
    Uses the LLM to intelligently select relevant tables from the available list,
    based on the user's natural language question.
    """
    prompt = (
        "Given the following list of database tables:\n"
        f"{', '.join(all_tables)}\n\n"
        f"And the user's question:\n\"{question}\"\n\n"
        "Return the names of the most relevant tables from the list. "
        "Only return a comma-separated list of table names. Do not include any explanation or extra text.\n"
    )

    try:
        response = llm(prompt)
        selected = [t.strip() for t in response.split(",") if t.strip() in all_tables]
        return selected or all_tables[:3]  # fallback if model gives invalid output
    except Exception as e:
        print(f"‚ö†Ô∏è LLM table selection failed: {e}")
        return all_tables[:3]

def get_schema_text(db: SQLDatabase, db_uri: str) -> str:
    """
    Build a compact, LLM-optimized schema representation for SQL generation.
    Includes the initially selected tables and any 1-hop foreign key-related tables.
    """
    engine = create_engine(db_uri)
    inspector = inspect(engine)

    # Start with tables included in the filtered DB
    initial_tables = set(db.get_usable_table_names())
    all_tables_to_include = set(initial_tables)

    # Step 1: Add 1-hop foreign key related tables
    for tbl in initial_tables:
        try:
            fks = inspector.get_foreign_keys(tbl)
            for fk in fks:
                referred_table = fk.get("referred_table")
                if referred_table:
                    all_tables_to_include.add(referred_table)
        except Exception:
            continue  # skip faulty FKs

    lines = []

    # Step 2: Format schema for all included tables
    for tbl in sorted(all_tables_to_include):
        try:
            cols = inspector.get_columns(tbl)
            col_defs = [f"{col['name']} {col['type']}" for col in cols]
            col_str = ", ".join(col_defs)
            lines.append(f"TABLE {tbl} ({col_str})")
        except Exception:
            lines.append(f"TABLE {tbl} ([Error reading columns])")

        try:
            fks = inspector.get_foreign_keys(tbl)
            for fk in fks:
                for col, ref_col in zip(fk.get("constrained_columns", []), fk.get("referred_columns", [])):
                    referred_table = fk.get("referred_table", "Unknown")
                    lines.append(f"FK {tbl}.{col} -> {referred_table}.{ref_col}")
        except Exception:
            lines.append(f"# [Error reading foreign keys for {tbl}]")

        lines.append("")  # Add a blank line between tables

    return "\n".join(lines).strip()


def extract_sql_query(text: str) -> str:
    """
    Extracts the SQL query from the model's response.
    Removes ```sql blocks and grabs the first valid SQL statement.
    """
    # Remove markdown-style SQL code block (```sql ... ```)
    text = text.strip()
    if text.startswith("```sql"):
        text = text.removeprefix("```sql").strip()
    if text.endswith("```"):
        text = text.removesuffix("```").strip()

    # Now extract the actual SQL query
    pattern = re.compile(r"(?i)(SELECT|INSERT|UPDATE|DELETE).*?;", re.DOTALL)
    match = pattern.search(text)
    if match:
        return match.group(0).strip()
    else:
        return text.strip()
def generate_sql_custom(question: str, schema_text: str, llm) -> str:
    """
    Manually constructs a prompt (similar to your base version) and uses the LLM
    to generate a SQL query.
    """
    prompt = (
        "Generate an SQL query strictly based on the schema provided.\n\n"
        f"Schema:\n{schema_text}\n\n"
        f"Question:\n{question}\n\n"
        "Only output SQL code. Do not output any explanation or additional text.\n"
        "SQL:"
    )
    result = llm(prompt)
    return result.strip()

from langchain_community.utilities import SQLDatabase
from sqlalchemy import create_engine, text


# ------------------------------------------------------------------
# Helper: run SQL and capture errors
# ------------------------------------------------------------------
def execute_sql(sql: str, db_uri: str):
    try:
        engine = create_engine(db_uri)
        with engine.connect() as conn:
            rows = conn.execute(text(sql)).fetchall()
        rows = [dict(r._mapping) for r in rows]
        return {"rows": rows}                # success
    except Exception as e:
        return {"error": str(e)}             # failure


# ------------------------------------------------------------------
# Main pipeline with single automatic retry
# ------------------------------------------------------------------
def process_question(question: str, db_uri: str, llm, max_retry: int = 1) -> dict:
    """
    1. Pick tables
    2. Build schema
    3. Ask LLM for SQL
    4. Execute SQL
    5. If DB error, ask LLM once more to fix it (bounded retry)
    Returns: {sql: str, results: list[dict]}  OR  {sql: str, error: str}
    """

    # ----- table discovery -----
    wide_db = SQLDatabase.from_uri(db_uri)
    all_table_names = wide_db.get_usable_table_names()
    relevant_tables = pick_tables(question, all_table_names, llm)   # <- fixed var

    # ----- schema text -----
    filtered_db = SQLDatabase.from_uri(db_uri, include_tables=relevant_tables)
    schema_text = get_schema_text(filtered_db, db_uri)

    # ----- first SQL generation -----
    sql = extract_sql_query(
        generate_sql_custom(question, schema_text, llm)
    )

    # ----- try executing -----
    result = execute_sql(sql, db_uri)

    # ----- automatic single retry if error -----
    if "error" in result and max_retry > 0:
        retry_prompt = (
            f"You generated this SQL:\n{sql}\n\n"
            f"It failed with this error:\n{result['error']}\n\n"
            f"Schema:\n{schema_text}\n\n"
            "Please correct the SQL. Output only SQL code."
        )
        fixed_sql = extract_sql_query(llm(retry_prompt))
        retry_out = execute_sql(fixed_sql, db_uri)

        # overwrite if second attempt succeeded (or keep final error)
        sql = fixed_sql
        result = retry_out

    # ----- build response -----
    if "rows" in result:
        return {"sql": sql, "results": result["rows"]}
    else:
        return {"sql": sql, "error": result["error"]}






























































































































































































































































































[DEBUG] Cypher generated:
MATCH (customer:Customer)-[:RELATED_TO]->(index:Index)
WHERE index.Country='Bhutan'
RETURN customer.Customer_Id AS CustomerId, customer.First_Name AS FirstName
ORDER BY customer.First_Name ASC
```
This Cypher query searches for customers by following the `RELATED_TO` relationship from the `Index` node and filters the result by the country property equal to 'Bhutan'. It then returns the `CustomerId` and `FirstName` properties of the matching `Customer` nodes in an ascending order by their `FirstName` property.
[ERROR] Cypher query failed: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '```\nThis Cypher query searches for customers by following the `': expected 'FOREACH', ',', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 5, column 1 (offset: 193))
"```"
^}
Answer: []


























[DEBUG] Cypher generated:

```cypher

MATCH (c:Customer)-[:RELATED_TO]->(i:Index)

WHERE i.Country = 'bhutan'

RETURN c.Customer_Id AS id, c.First_Name AS name

```

This query will return the Customer nodes that have a relationship `RELATED_TO` to an Index node with the property `Country` equal to 'bhutan'. The results will include the `Customer_Id` and `First_Name` properties of the Customer node.

[ERROR] Cypher query failed: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '```cypher\nMATCH (c:Customer)-[:RELATED_TO]->(i:Index)\nWHERE i.Country = 'bhutan'\nRETURN c.Customer_Id AS id, c.First_Name AS name\n```': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 1, column 1 (offset: 0))

"```cypher"

^}
 























ok lets do this functionality . create functions 
step 1 i want the user to like upload a csv file from CMD
step 2 the csv file gets converted into this GRAPH DB ( stored in GRAPH DB )
Step 3 i have a mistral model which runs locally it should dynamically know the schema of the GRAPH DB and make a cyper query according to the question asked by the user on the csv 
step 4 run the cypher query , fetch the results
step 5 . display the results 

import pandas as pd
from neo4j import GraphDatabase
from llama_cpp import Llama
 
# === Neo4j Configuration ===
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "your_password"  # Replace with your actual password
 
# === Load CSV ===
def load_csv(file_path):
    df = pd.read_csv(file_path)
    df.columns = [col.strip().replace(" ", "_") for col in df.columns]
    print("[INFO] CSV Loaded. Columns:", list(df.columns))
    return df
 
# === Infer Simple Schema ===
def infer_schema(df):
    node1, node2 = df.columns[0], df.columns[1]
    rel_prop = df.columns[2] if len(df.columns) > 2 else None
    return {
        "node1": node1,
        "node2": node2,
        "rel_type": "RELATED_TO",
        "rel_prop": rel_prop
    }
 
# === Insert Data into Neo4j ===
def insert_into_neo4j(df, schema):
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    with driver.session() as session:
        for _, row in df.iterrows():
            props = f"{{{schema['rel_prop']}: '{row[schema['rel_prop']]}'}}" if schema['rel_prop'] else ""
            cypher = f"""
            MERGE (a:{schema['node1']} {{name: $a_name}})
            MERGE (b:{schema['node2']} {{name: $b_name}})
            MERGE (a)-[:{schema['rel_type']} {props}]->(b)
            """
            session.run(
                cypher,
                a_name=str(row[schema['node1']]),
                b_name=str(row[schema['node2']])
            )
    print("[INFO] Data inserted into Neo4j.")
 
# === Prompt Builder for Mistral ===
def build_prompt(schema, question):
    return f"""You are a Cypher expert working with a Neo4j graph database.
Graph Schema:
(:{schema['node1']})-[:{schema['rel_type']}]->(:{schema['node2']})
Properties:
- {schema['node1']}: name
- {schema['node2']}: name
- {schema['rel_type']}: {schema['rel_prop']}
 
Examples:
Q: What did Alice buy?
A: MATCH (a:{schema['node1']} {{name: "Alice"}})-[:{schema['rel_type']}]->(b:{schema['node2']}) RETURN b.name
Q: {question}
A:"""
 
# === Query Mistral via llama-cpp ===
llm = Llama(
    model_path="mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=6,
    verbose=True
)
 
def query_mistral(prompt):
    output = llm(prompt, max_tokens=200, stop=["\n\n", "\nQ:"], echo=False)
    return output["choices"][0]["text"].strip()
 
# === Execute Cypher Query ===
def run_cypher_query(cypher):
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    with driver.session() as session:
        result = session.run(cypher)
        return [record.values() for record in result]
 
# === Main ===
def main():
    csv_path = input("Enter full path of your CSV file (or just filename if it's in the same folder): ")
    df = load_csv(csv_path)
    schema = infer_schema(df)
    insert_into_neo4j(df, schema)
 
    while True:
        question = input("\nAsk a question (or type 'exit' to quit): ")
        if question.lower() == 'exit':
            break
        prompt = build_prompt(schema, question)
        print("\n[DEBUG] Prompt sent to Mistral:\n", prompt)
        cypher = query_mistral(prompt)
        print("\n[DEBUG] Cypher generated:\n", cypher)
        results = run_cypher_query(cypher)
        print("Answer:", results)
 
if __name__ == "__main__":
    main()










































































































































































































































Application bundle generation failed. [2.648 seconds]

‚ñ≤ [WARNING] NG8103: The `*ngFor` directive was used in the template, but neither the `NgFor` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @for or make sure that either the `NgFor` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:5:10:
      5 ‚îÇ     <div *ngFor="let msg of messages">
        ‚ïµ           ~~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


‚ñ≤ [WARNING] NG8103: The `*ngIf` directive was used in the template, but neither the `NgIf` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @if or make sure that either the `NgIf` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:6:29:
      6 ‚îÇ       <div class="user-msg" *ngIf="msg.role === 'user'">You: {{ msg...
        ‚ïµ                              ~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


‚ñ≤ [WARNING] NG8103: The `*ngIf` directive was used in the template, but neither the `NgIf` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @if or make sure that either the `NgIf` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:8:28:
      8 ‚îÇ       <div class="bot-msg" *ngIf="msg.role === 'bot'">
        ‚ïµ                             ~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


‚ñ≤ [WARNING] NG8103: The `*ngIf` directive was used in the template, but neither the `NgIf` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @if or make sure that either the `NgIf` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:12:16:
      12 ‚îÇ         <table *ngIf="msg.results?.length">
         ‚ïµ                 ~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


‚ñ≤ [WARNING] NG8103: The `*ngFor` directive was used in the template, but neither the `NgFor` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @for or make sure that either the `NgFor` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:15:19:
      15 ‚îÇ               <th *ngFor="let key of getKeys(msg.results[0])">{{ k...
         ‚ïµ                    ~~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


‚ñ≤ [WARNING] NG8103: The `*ngFor` directive was used in the template, but neither the `NgFor` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @for or make sure that either the `NgFor` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:19:17:
      19 ‚îÇ             <tr *ngFor="let row of msg.results">
         ‚ïµ                  ~~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


‚ñ≤ [WARNING] NG8103: The `*ngFor` directive was used in the template, but neither the `NgFor` directive nor the `CommonModule` was imported. Use Angular's built-in control flow @for or make sure that either the `NgFor` directive or the `CommonModule` is included in the `@Component.imports` array of this component. [plugin angular-compiler]

    src/app/app.component.html:20:19:
      20 ‚îÇ               <td *ngFor="let key of getKeys(row)">{{ row[key] }}<...
         ‚ïµ                    ~~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


X [ERROR] NG8002: Can't bind to 'ngModel' since it isn't a known property of 'input'. [plugin angular-compiler]

    src/app/app.component.html:29:11:
      29 ‚îÇ     <input [(ngModel)]="userInput" placeholder="Ask your question....
         ‚ïµ            ~~~~~~~~~~~~~~~~~~~~~~~

  Error occurs in the template of component AppComponent.

    src/app/app.component.ts:13:15:
      13 ‚îÇ   templateUrl: './app.component.html',
         ‚ïµ                ~~~~~~~~~~~~~~~~~~~~~~


Watch mode enabled. Watching for file changes...


































































User Query: give me eon orders from 9am to 12pm on 1st june 2015
Llama.generate: 1 prefix-match hit, remaining 788 prompt tokens to eval
llama_perf_context_print:        load time =    1240.90 ms
llama_perf_context_print: prompt eval time =   80236.62 ms /   788 tokens (  101.82 ms per token,     9.82 tokens per second)
llama_perf_context_print:        eval time =   16153.41 ms /    99 runs   (  163.17 ms per token,     6.13 tokens per second)
llama_perf_context_print:       total time =   96451.71 ms /   887 tokens


User Query: give me the orders of eon from 9am -12pm on 1st june2015
Llama.generate: 761 prefix-match hit, remaining 28 prompt tokens to eval
llama_perf_context_print:        load time =    1240.90 ms
llama_perf_context_print: prompt eval time =    2154.85 ms /    28 tokens (   76.96 ms per token,    12.99 tokens per second)
llama_perf_context_print:        eval time =   12370.32 ms /    92 runs   (  134.46 ms per token,     7.44 tokens per second)
llama_perf_context_print:       total time =   14563.50 ms /   120 tokens





















select DISTINCT si.order_no ,si.item_no ,ii.action as 'Order Action',oo.order_type as 'Order Type',sca.account_no as 'BAN',sca.account_name as 'CustName',
itc.description ,psp.sub_profile_desc, si.circuit_id as 'FroId', scp.cpi_status_code as 'Circuit Status',     	
case when sva.cust_site_id != 'null' then  aa.address + '  '+ aa.city + ' '+ aa.state + ' '+ aa.country else '' end as 'Customer Prem Address A',  
case when sva.cust_site_id != 'null' then  ca.country_name else '' end as 'A customer prem country',  
case when svz.cust_site_id != 'null' then  az.address + '  '+ az.city + ' '+ az.state + ' '+ az.country else '' end as 'Customer Prem Address Z',  
case when svz.cust_site_id != 'null' then  cz.country_name else '' end as 'Z customer prem country',  ii.create_date as 'Created Date'
from orders oo join sonet_item si on oo.id=si.order_no  
join improv_item ii on ii.id=si.id  
join sonet_vendor_interface sva on sva.side='A' and sva.item_id=si.id  
left outer join site sa on sva.cust_site_id=sa.site_id  
left outer join address aa on sa.address_id=aa.address_id  
join sonet_vendor_interface svz on svz.side='Z'and svz.item_id=si.id  
left outer join site sz on svz.cust_site_id=sz.site_id  
left outer join address az on sz.address_id=az.address_id  
left outer join country ca on ca.country_alpha3_code=aa.country  
left outer join country cz on cz.country_alpha3_code=az.country , 
profile_sub_profile psp,improv_item_catalog itc,sonet_customer_account sca,sonet_cpi scp  
where si.sub_profile_code =psp.id and sca.account_no=oo.account_no and si.circuit_id=scp.circuit_id and itc.item_type=psp.item_type  and
ii.create_date between '2024/10/01' and '2024/10/24';



























































User Query: hello i want orders which are cancelled
Llama.generate: 676 prefix-match hit, remaining 12 prompt tokens to eval
llama_perf_context_print:        load time =   61542.72 ms
llama_perf_context_print: prompt eval time =     985.22 ms /    12 tokens (   82.10 ms per token,    12.18 tokens per second)
llama_perf_context_print:        eval time =    9462.99 ms /    66 runs   (  143.38 ms per token,     6.97 tokens per second)
llama_perf_context_print:       total time =   10473.24 ms /    78 tokens

LLM Response (raw):
{
  "source_system": "EON",
  "order_type": "ALL",
  "order_status": "cancelled",
  "order_action": "ALL",
  "start_date": "none",
  "end_date": "now"
}

llama_perf_context_print:       total time =   10473.24 ms /    78 tokens

LLM Response (raw):
{
  "source_system": "EON",
  "order_type": "ALL",
  "order_status": "cancelled",
  "order_action": "ALL",
  "start_date": "none",
  "end_date": "now"
}

LLM Response (raw):
{
  "source_system": "EON",
  "order_type": "ALL",
  "order_status": "cancelled",
  "order_action": "ALL",
  "start_date": "none",
  "end_date": "now"
}

  "source_system": "EON",
  "order_type": "ALL",
  "order_status": "cancelled",
  "order_action": "ALL",
  "start_date": "none",
  "end_date": "now"
}

  "order_type": "ALL",
  "order_status": "cancelled",
  "order_action": "ALL",
  "start_date": "none",
  "end_date": "now"
}

  "order_action": "ALL",
  "start_date": "none",
  "end_date": "now"
}

  "start_date": "none",
  "end_date": "now"
}

  "end_date": "now"
}

}


































PS C:\Users\AD54619\Text2Sql\approach1> python new.py
‚è≥ Loading Mistral 7B model via llama.cpp...
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 58, in <module>
    llm = MistralLLM(model_path=MODEL_PATH)
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 29, in __init__
    self.verbose = verbose  # ‚úÖ Fix: Add this line
    ^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\pydantic\main.py", line 991, in __setattr__
    setattr_handler(self, name, value)  # call here to not memo on possibly unknown fields
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\pydantic\main.py", line 99, in _model_field_setattr_handler  
    model.__pydantic_fields_set__.add(name)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 53, in __getattr__
    raise AttributeError(f"{type(self).__name__!r} object has no attribute {name!r}")
AttributeError: 'MistralLLM' object has no attribute '__pydantic_fields_set__'. Did you mean: '__pydantic_fields__'? 




































üß† Step: get me list of all customers
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 85, in <module>
    chat()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 81, in chat
    for step in agent_executor.stream({"messages": [HumanMessage(content=user_input)]}, stream_mode="values"):
                ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\pregel\__init__.py", line 2340, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        loop.tasks.values(),
        ^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        get_waiter=get_waiter,
        ^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\pregel\runner.py", line 158, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<11 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\pregel\retry.py", line 40, in run_with_retry       
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\utils\runnable.py", line 606, in invoke
    input = step.invoke(input, config, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\utils\runnable.py", line 363, in invoke
    ret = context.run(self.func, *args, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 745, in call_model
    response = cast(AIMessage, model_runnable.invoke(state, config))
                               ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\runnables\base.py", line 3025, in invoke      
    input = context.run(step.invoke, input, config)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 307, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 843, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 664, in generate
    self.verbose,
    ^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 51, in __getattr__
    raise AttributeError(f"{type(self).__name__!r} object has no attribute {name!r}")
AttributeError: 'MistralLLM' object has no attribute 'verbose'
During task with name 'agent' and id '05dbd901-99e5-f529-6d83-b3f81d8644da'







































üß† Step: list all customers
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 82, in <module>
    chat()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 78, in chat
    for step in agent_executor.stream({"messages": [HumanMessage(content=user_input)]}, stream_mode="values"):
                ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\pregel\__init__.py", line 2340, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        loop.tasks.values(),
        ^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        get_waiter=get_waiter,
        ^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\pregel\runner.py", line 158, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<11 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\pregel\retry.py", line 40, in run_with_retry       
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\utils\runnable.py", line 606, in invoke
    input = step.invoke(input, config, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\utils\runnable.py", line 363, in invoke
    ret = context.run(self.func, *args, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 745, in call_model
    response = cast(AIMessage, model_runnable.invoke(state, config))
                               ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\runnables\base.py", line 3025, in invoke      
    input = context.run(step.invoke, input, config)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 307, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 843, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 663, in generate
    self.callbacks,
    ^^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\pydantic\main.py", line 984, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'MistralLLM' object has no attribute 'callbacks'
During task with name 'agent' and id 'd47a6d5a-987a-ea77-1805-3ebbd8e47d2d'
































be provided when using hosted LangSmith API
  warnings.warn(
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 62, in <module>
    agent_executor = create_react_agent(llm, tools, prompt=system_prompt)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 160, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 691, in create_react_agent
    model = cast(BaseChatModel, model).bind_tools(tool_classes)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1174, in
 bind_tools
    raise NotImplementedError
NotImplementedError
PS C:\Users\AD54619\Text2Sql\approach1> 























Guessed chat format: mistral-instruct
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 50, in <module>
    llm = MistralLLM(model_path=MODEL_PATH)
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 31, in __init__
    self.llama = Llama(
    ^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\pydantic\main.py", line 990, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\pydantic\main.py", line 1037, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MistralLLM" object has no field "llama"






















C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langsmith\client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API
  warnings.warn(
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 54, in <module>
    agent_executor = create_react_agent(llm, tools, prompt=system_prompt)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 160, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 691, in create_react_agent
    model = cast(BaseChatModel, model).bind_tools(tool_classes)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MistralLLM' object has no attribute 'bind_tools'
PS C:\Users\AD54619\Text2Sql\approach1> python new.py
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 46, in <module>
    llm = MistralLLM(model_path=MODEL_PATH)
TypeError: Can't instantiate abstract class MistralLLM without an implementation for abstract methods '_generate', '_llm_type'
PS C:\Users\AD54619\Text2Sql\a























Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 54, in <module>
    agent_executor = create_react_agent(llm, tools, prompt=system_prompt)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 160, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langgraph\prebuilt\chat_agent_executor.py", line 691, in create_react_agent
    model = cast(BaseChatModel, model).bind_tools(tool_classes)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MistralLLM' object has no attribute 'bind_tools'






Guessed chat format: mistral-instruct
‚úÖ Model loaded!
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\approach1\new.py", line 40, in <module>
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\pydantic\main.py", line 243, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for SQLDatabaseToolkit
llm
  Input should be a valid dictionary or instance of BaseLanguageModel [type=model_type, input_value=<__main__.MistralLLM object at 0x0000013CEB299940>, input_type=MistralLLM]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type

















POST https://localhost:5000/api/query
Error: write EPROTO 54290880:error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER:..\..\..\..\src\third_party\boringssl\src\ssl\tls_record.cc:231:
Request Headers
Content-Type: application/json
User-Agent: PostmanRuntime/7.43.0
Accept: */*
Postman-Token: 90760198-d809-4294-951b-93166c45efd1
Host: localhost:5000
Accept-Encoding: gzip, deflate, br
Connection: keep-alive
Request Body






















User Question: get me employee details who belong to department engineering
Llama.generate: 201 prefix-match hit, remaining 23 prompt tokens to eval
llama_perf_context_print:        load time =   19596.31 ms
llama_perf_context_print: prompt eval time =    1581.68 ms /    23 tokens (   68.77 ms per token,    14.54 tokens per second)
llama_perf_context_print:        eval time =    8347.60 ms /    64 runs   (  130.43 ms per token,     7.67 tokens per second)
llama_perf_context_print:       total time =    9951.81 ms /    87 tokens

Final SQL Query:
```sql
SELECT employees.id, employees.name, employees.age, departments.department_name
FROM employees
JOIN departments ON employees.department = departments.department_name
```


‚ùå Error executing query: (pymysql.err.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '```sql\nSELECT employees.id, employees.name, employees.age, departments.departmen' at line 1")
[SQL: ```sql
SELECT employees.id, employees.name, employees.age, departments.department_name
FROM employees
JOIN departments ON employees.department = departments.department_name
WHERE departments.department_name = 'engineering'
```]
(Background on this error at: https://sqlalche.me/e/20/f405)

User Question:





















PS C:\Users\AD54619\Text2Sql> python gemma_sql.py
‚è≥ Loading Gemma-2B model (HuggingFace style)...
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.36it/s]
‚úÖ Model loaded!
Device set to use cpu

üîπGemma-2B Chat w/ MySQL using custom prompt (no chain-of-thought).

Type 'exit' or 'quit' to stop.

User Question: fetch details of employee belonging to sales department
C:\Users\AD54619\Text2Sql\gemma_sql.py:126: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.
  result = llm(prompt)

Final SQL Query:
Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
fetch details of employee belonging to sales department

Only output SQL code. Do not output any explanation or additional text.
SQL:

SELECT
  e.id,
  e.name,
  e.age,
  e.salary,
  d.department_name
FROM
  employees e
  LEFT JOIN departments d ON e.department = d.id
WHERE
  d.department_name ='sales'

Output:
SELECT e.id, e.name, e.age, e.salary, d.department_name FROM employees e LEFT JOIN departments d ON e.department = d.id WHERE d.department_name ='sales'


‚ùå Error executing query: (pymysql.err.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Generate an SQL query strictly based on the schema provided.\n\nSchema:\nTable: dep' at line 1")
[SQL: Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
fetch details of employee belonging to sales department

Only output SQL code. Do not output any explanation or additional text.
SQL:

SELECT
  e.id,
  e.name,
  e.age,
  e.salary,
  d.department_name
FROM
  employees e
  LEFT JOIN departments d ON e.department = d.id
WHERE
  d.department_name ='sales'

Output:
SELECT e.id, e.name, e.age, e.salary, d.department_name FROM employees e LEFT JOIN departments d ON e.department = d.id WHERE d.department_name ='sales']
(Background on this error at: https://sqlalche.me/e/20/f405)

User Question: get employees earning more than 20000

Final SQL Query:
SELECT *
FROM employees
WHERE salary > 20000;

DB Results:
(101, 'John Smith', 35, '1', 85000)
(102, 'Sarah Johnson', 42, '2', 78000)
(103, 'Michael Brown', 28, '1', 72000)
(104, 'Jessica Davis', 31, '3', 90000)
(105, 'Robert Wilson', 45, '4', 65000)
(106, 'Lisa Anderson', 38, '5', 92000)
(107, 'David Miller', 27, '1', 68000)
(108, 'Jennifer Garcia', 33, '2', 71000)
(109, 'Thomas Martinez', 41, '3', 86000)
(110, 'Emily Rodriguez', 29, '5', 81000)

User Question: get employee whose name begins with letter R 

Final SQL Query:
Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
get employee whose name begins with letter R

Only output SQL code. Do not output any explanation or additional text.
SQL:


‚ùå Error executing query: (pymysql.err.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Generate an SQL query strictly based on the schema provided.\n\nSchema:\nTable: dep' at line 1")
[SQL: Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
get employee whose name begins with letter R

Only output SQL code. Do not output any explanation or additional text.
SQL:]
(Background on this error at: https://sqlalche.me/e/20/f405)

User Question: get employee who belongs to finance department

Final SQL Query:
Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
get employee who belongs to finance department

Only output SQL code. Do not output any explanation or additional text.
SQL:


‚ùå Error executing query: (pymysql.err.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Generate an SQL query strictly based on the schema provided.\n\nSchema:\nTable: dep' at line 1")
[SQL: Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
get employee who belongs to finance department

Only output SQL code. Do not output any explanation or additional text.
SQL:]
(Background on this error at: https://sqlalche.me/e/20/f405)

User Question: get employee who earn more than 20000 incomme

Final SQL Query:
Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
get employee who earn more than 20000 incomme

Only output SQL code. Do not output any explanation or additional text.
SQL:

SELECT *
FROM employees
WHERE salary > 20000


‚ùå Error executing query: (pymysql.err.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Generate an SQL query strictly based on the schema provided.\n\nSchema:\nTable: dep' at line 1")
[SQL: Generate an SQL query strictly based on the schema provided.

Schema:
Table: departments
  - id (INTEGER)
  - department_name (TEXT)
  - location (TEXT)

Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
get employee who earn more than 20000 incomme

Only output SQL code. Do not output any explanation or additional text.
SQL:

SELECT *
FROM employees
WHERE salary > 20000]
(Background on this error at: https://sqlalche.me/e/20/f405)

User Question:































Type 'exit' or 'quit' to stop.

User Question: fetch all employees whoose salary is more than 50000
C:\Users\AD54619\Text2Sql\simple_tiny.py:94: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.
  result = llm(prompt)

Generated SQL Query:
Generate an SQL query strictly based on the schema provided.

Schema:
Table: employees
  - id (INTEGER)
  - name (TEXT)
  - age (INTEGER)
  - department (TEXT)
  - salary (INTEGER)

Question:
fetch all employees whoose salary is more than 50000

Only output SQL code. Do not output any explanation or additional text.
SQL:
SELECT * FROM employees WHERE salary > 50000;

Explanation:
This query fetches all employees whose salary is more than 50000.

I hope this helps! Let me know if you have any further questions.


‚ùå Error executing query: Not an executable object: 'Generate an SQL query strictly based on the schema provided.\n\nSchema:\nTable: employees\n  - id (INTEGER)\n  - name (TEXT)\n  - age (INTEGER)\n  - department (TEXT)\n  - salary (INTEGER)\n\nQuestion:\nfetch all employees whoose salary is more than 50000\n\nOnly output SQL code. Do not output any explanation or additional text.\nSQL:\nSELECT * FROM employees WHERE salary > 50000;\n\nExplanation:\nThis query fetches all employees whose salary is more than 50000.\n\nI hope this helps! Let me know if you have any further questions.'

User Question:




























KeyboardInterrupt
PS C:\Users\AD54619\Text2Sql> python simple_tiny.py
‚è≥ Loading TinyLlama model (HuggingFace style)...
‚úÖ Model loaded!
Device set to use cpu

üîπTinyLlama Chat w/ MySQL using custom prompt (no chain-of-thought).

Type 'exit' or 'quit' to stop.

User Question: give me employees whoose salary is more than 50000
C:\Users\AD54619\Text2Sql\simple_tiny.py:84: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.
  result = llm(prompt)

Generated SQL Query:
Generate an SQL query strictly based on the schema provided.

Schema:
Table: employees
  - [Error retrieving columns]

Question:
give me employees whoose salary is more than 50000

Only output SQL code. Do not output any explanation or additional text.
SQL:
SELECT * FROM employees
WHERE salary > 50000;


‚ùå Error executing query: Not an executable object: 'Generate an SQL query strictly based on the schema provided.\n\nSchema:\nTable: employees\n  - [Error retrieving columns]\n\nQuestion:\ngive me employees whoose salary is more than 50000\n\nOnly output SQL code. Do not output any explanation or additional text.\nSQL:\nSELECT * FROM employees\nWHERE salary > 50000;'

User Question:
















PS C:\Users\AD54619\Text2Sql> python better_tiny.py
‚è≥ Loading TinyLlama model (HuggingFace style)...
‚úÖ Model loaded!
Device set to use cpu

üîπTinyLlama Chat w/ MySQL using LangChain.

Type 'exit' or 'quit' to stop.

[DEBUG] Table Names: ['departments', 'employees']
User Question: which employees belong to finance department
[DEBUG] Using these tables: ['employees']
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 119, in <module>
    main()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 103, in main
    chain = create_sql_query_chain(
        llm=llm,
        db=filtered_db,
        prompt=custom_prompt
    )
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain\chains\sql_database\query.py", line 122, in create_sql_query_chain  
    raise ValueError(
    ...<3 lines>...
    )
ValueError: Prompt must have input variables: 'input', 'top_k', 'table_info'. Received prompt with input variables: ['question']. Full prompt:

input_variables=['question'] input_types={} partial_variables={} template='Generate a valid MySQL query for the following question:\n{question}\n\nReturn ONLY the SQL query. No extra explanation or examples.'


























S C:\Users\AD54619\Text2Sql> python better_tiny.py
‚è≥ Loading TinyLlama model (HuggingFace style)...
‚úÖ Model loaded!
Device set to use cpu

üîπ TinyLlama MySQL Demo (Raw DB results only). Type 'exit' or 'quit' to stop.

Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 115, in <module>
    main()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 84, in main
    columns = db.get_table_info(tbl)
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain_community\utilities\sql_database.py", line 322, in get_table_info   
    raise ValueError(f"table_names {missing_tables} not found in database")
ValueError: table_names {'a', 't', 'p', 'e', 'n', 'm', 'd', 's', 'r'} not found in database























User Question: employee name whose salary is more than 50000
[DEBUG] Using tables: ['departments', 'employees']
[chain/start] [chain:RunnableSequence] Entering Chain run with input:
{
  "input": "employee name whose salary is more than 50000"
}
[chain/start] [chain:RunnableSequence > chain:RunnableAssign<input,table_info>] Entering Chain run with input:
{
  "input": "employee name whose salary is more than 50000"
}
[chain/start] [chain:RunnableSequence > chain:RunnableAssign<input,table_info> > chain:RunnableParallel<input,table_info>] Entering Chain run with input:
{
  "input": "employee name whose salary is more than 50000"
}
[chain/start] [chain:RunnableSequence > chain:RunnableAssign<input,table_info> > chain:RunnableParallel<input,table_info> > chain:RunnableLambda] Entering Chain run with input:
{
  "input": "employee name whose salary is more than 50000"
}
[chain/start] [chain:RunnableSequence > chain:RunnableAssign<input,table_info> > chain:RunnableParallel<input,table_info> > chain:RunnableLambda] Entering Chain run with input:
{
  "input": "employee name whose salary is more than 50000"
}
[chain/end] [chain:RunnableSequence > chain:RunnableAssign<input,table_info> > chain:RunnableParallel<input,table_info> > chain:RunnableLambda] [8ms] Exiting Chain run with output:
{
  "output": "\nCREATE TABLE departments (\n\tid INTEGER NOT NULL, \n\tdepartment_name TEXT NOT NULL, \n\tlocation TEXT, \n\tPRIMARY KEY (id)\n)COLLATE utf8mb4_0900_ai_ci ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n\n/*\n3 rows from departments table:\nid\tdepartment_name\tlocation\n1\tEngineering\tNew York\n2\tMarketing\tChicago\n3\tFinance\tBoston\n*/\n\n\nCREATE TABLE employees (\n\tid INTEGER NOT NULL, \n\tname TEXT NOT NULL, \n\tage INTEGER, \n\tdepartment TEXT, \n\tsalary INTEGER, \n\tPRIMARY KEY (id)\n)COLLATE utf8mb4_0900_ai_ci ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n\n/*\n3 rows from employees table:\nid\tname\tage\tdepartment\tsalary\n101\tJohn Smith\t35\t1\t85000\n102\tSarah Johnson\t42\t2\t78000\n103\tMichael Brown\t28\t1\t72000\n*/"
}
[chain/error] [chain:RunnableSequence > chain:RunnableAssign<input,table_info> > chain:RunnableParallel<input,table_info> > chain:RunnableLambda] [57ms] Chain run errored with error:
"KeyError('question')Traceback (most recent call last):\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1925, in _call_with_config\n    context.run(\n    ~~~~~~~~~~~^\n        call_func_with_variable_args,  # type: ignore[arg-type]\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ),\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 430, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4573, in _invoke\n    output = call_func_with_variable_args(\n        self.func, input, config, run_manager, **kwargs\n    )\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 430, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain\\chains\\sql_database\\query.py\", line 131, in <lambda>\n    \"input\": lambda x: x[\"question\"] + \"\\nSQLQuery: \",\n                       ~^^^^^^^^^^^^\n\n\nKeyError: 'question'"
[chain/error] [chain:RunnableSequence > chain:RunnableAssign<input,table_info> > chain:RunnableParallel<input,table_info>] [94ms] Chain run errored with error:
"KeyError('question')Traceback (most recent call last):\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3728, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             
      ~~~~~~~~~~~~~^^\n\n\n  File \"C:\\Users\\AD54619\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n\n\n  File \"C:\\Users\\AD54619\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n\n\n  File \"C:\\Users\\AD54619\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3712, in _invoke_step\n    return context.run(\n           ~~~~~~~~~~~^\n        step.invoke,\n        ^^^^^^^^^^^^\n        input,\n        ^^^^^^\n        child_config,\n        ^^^^^^^^^^^^^\n    )\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4719, in invoke\n    return self._call_with_config(\n           ~~~~~~~~~~~~~~~~~~~~~~^\n        self._invoke,\n        ^^^^^^^^^^^^^\n    ...<2 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1925, in _call_with_config\n    context.run(\n    ~~~~~~~~~~~^\n        call_func_with_variable_args,  # type: ignore[arg-type]\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ),\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 430, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4573, in _invoke\n    output = call_func_with_variable_args(\n        self.func, input, config, run_manager, **kwargs\n    )\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 430, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain\\chains\\sql_database\\query.py\", line 131, in <lambda>\n    \"input\": lambda x: x[\"question\"] + \"\\nSQLQuery: \",\n                       ~^^^^^^^^^^^^\n\n\nKeyError: 'question'"
[chain/error] [chain:RunnableSequence > chain:RunnableAssign<input,table_info>] [116ms] Chain run errored with error:
"KeyError('question')Traceback (most recent call last):\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1925, in _call_with_config\n    context.run(\n    ~~~~~~~~~~~^\n        call_func_with_variable_args,  # type: ignore[arg-type]\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ),\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 430, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py\", line 483, in _invoke\n    **self.mapper.invoke(\n      ~~~~~~~~~~~~~~~~~~^\n        input,\n        ^^^^^^\n        patch_config(config, callbacks=run_manager.get_child()),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **kwargs,\n        ^^^^^^^^^\n    ),\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3728, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ~~~~~~~~~~~~~^^\n\n\n  File \"C:\\Users\\AD54619\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n\n\n  File \"C:\\Users\\AD54619\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n\n\n  File \"C:\\Users\\AD54619\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3712, in _invoke_step\n    return context.run(\n           ~~~~~~~~~~~^\n        step.invoke,\n        ^^^^^^^^^^^^\n        input,\n        ^^^^^^\n        child_config,\n        ^^^^^^^^^^^^^\n    )\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4719, in invoke\n    return self._call_with_config(\n           ~~~~~~~~~~~~~~~~~~~~~~^\n        self._invoke,\n        ^^^^^^^^^^^^^\n    ...<2 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1925, in _call_with_config\n    context.run(\n    ~~~~~~~~~~~^\n        call_func_with_variable_args,  # type: ignore[arg-type]\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ),\n    ^\n\n\n  File \"C:\\Users\\AD54619\\Text2Sql\\virtualen\\Lib\\site-packages\\lang
































Type 'exit' or 'quit' to stop.

User Question: give me employee name whose salary is more than 50000
[DEBUG] Using tables: ['departments', 'employees']
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 132, in <module>
    main()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 108, in main
    chain = create_sql_query_chain(
        llm=llm,
    ...<2 lines>...
        top_k=3
    )
TypeError: create_sql_query_chain() got an unexpected keyword argument 'top_k'






























----
PS C:\Users\AD54619\Text2Sql> python better_tiny.py
‚è≥ Loading TinyLlama model (HuggingFace style)...
‚úÖ Model loaded!
Device set to use cpu

üîπTinyLlama Chat w/ MySQL (Only raw DB data printed)

Type 'exit' or 'quit' to stop.

User Question: which employee had more than 5000 salary 
[DEBUG] Using tables: ['departments', 'employees']
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 132, in <module>
    main()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 104, in main
    chain = create_sql_query_chain(
        llm=llm,
    ...<2 lines>...
        # no 'output_parser' argument here, since your version doesn't support it
    )
  File "C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\langchain\chains\sql_database\query.py", line 122, in create_sql_query_chain  
    raise ValueError(
    ...<3 lines>...
    )
ValueError: Prompt must have input variables: 'input', 'top_k', 'table_info'. Received prompt with input variables: ['question']. Full prompt:

input_variables=['question'] input_types={} partial_variables={} template='Generate a valid MySQL query for the user question:\n{question}\n\nOnly output the SQL query. Nothing else.'

















üîπTinyLlama Chat w/ MySQL (Only raw DB data printed)

Type 'exit' or 'quit' to stop.

C:\Users\AD54619\Text2Sql\better_tiny.py:79: LangChainDeprecationWarning: The class `QuerySQLDataBaseTool` was deprecated in LangChain 0.3.12 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-community package and should be used instead. To use it run `pip install -U :class:`~langchain-community` and import as `from :class:`~langchain_community.tools import QuerySQLDatabaseTool``.
  query_tool = QuerySQLDataBaseTool(db=wide_db)
User Question: which employee has more than 50000 salary
[DEBUG] Using tables: ['departments', 'employees']
Traceback (most recent call last):
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 129, in <module>
    main()
    ~~~~^^
  File "C:\Users\AD54619\Text2Sql\better_tiny.py", line 99, in main
    chain = create_sql_query_chain(
        llm=llm,
    ...<2 lines>...
        output_parser=StrOutputParser(),  # ensures raw string
    )
TypeError: create_sql_query_chain() got an unexpected keyword argument 'output_parser'





    
    sql_query = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    return sql_query

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate SQL from natural language')
    parser.add_argument('--question', type=str, help='The natural language question')
    parser.add_argument('--schema_file', type=str, help='File containing the database schema')
    
    args = parser.parse_args()
    
    if args.question and args.schema_file:
        with open(args.schema_file, 'r') as f:
            schema = f.read()
        
        sql = generate_sql(args.question, schema)
        print("\nGenerated SQL Query:")
        print(sql)
    else:
        # Use the example from your code if no arguments provided
        question = "What is the average, minimum, and maximum age for all French musicians?"
        schema = """
           "stadium" "Stadium_ID" int , "Location" text , "Name" text , "Capacity" int , "Highest" int , "Lowest" int , "Average" int , foreign_key:  primary key: "Stadium_ID" [SEP] "singer" "Singer_ID" int , "Name" text , "Country" text , "Song_Name" text , "Song_release_year" text , "Age" int , "Is_male" bool , foreign_key:  primary key: "Singer_ID" [SEP] "concert" "concert_ID" int , "concert_Name" text , "Theme" text , "Year" text , foreign_key: "Stadium_ID" text from "stadium" "Stadium_ID" , primary key: "concert_ID" [SEP] "singer_in_concert"  foreign_key: "concert_ID" int from "concert" "concert_ID" , "Singer_ID" text from "singer" "Singer_ID" , primary key: "concert_ID" "Singer_ID"
        """
        
        sql = generate_sql(question, schema)
        print("\nGenerated SQL Query:")
        print(sql)


PS C:\Users\AD54619\Text2Sql> python load.py --question "What is the average, minimum, and maximum age for all French musicians?" --schema_file schema.txt
Processing query...
C:\Users\AD54619\Text2Sql\virtualen\Lib\site-packages\transformers\generation\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(

Generated SQL Query:
SELECT avg(age), min(age), max(age) FROM singer WHERE country = 'France'
