from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ðŸ”¹ Step 1: Manually Login using Access Token
HUGGINGFACE_TOKEN = ""  # Replace with your token
login(HUGGINGFACE_TOKEN)

# ðŸ”¹ Step 2: Load the Model and Tokenizer (on CPU)
MODEL_NAME = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map="cpu")

# ðŸ”¹ Step 3: Hardcoded Schema
SCHEMA = """
Tables:
1. employees (id INT, name TEXT, age INT, department TEXT, salary INT, hire_date DATE)
2. departments (id INT, department_name TEXT, location TEXT)

Relations:
- employees.department is a foreign key referring to departments.id
"""

# ðŸ”¹ Step 4: Interactive Loop
print("\nðŸ”¹ AI is ready! Ask questions about the schema below.")
print("ðŸ›‘ Type 'exit' to quit.\n")

while True:
    question = input("ðŸ’¬ Ask your question: ")
    
    if question.lower() in ["exit", "quit"]:
        print("ðŸ‘‹ Exiting... Goodbye!")
        break

    # ðŸ”¹ Step 5: Format Prompt with Schema
    input_text = f"Schema:\n{SCHEMA}\n\nQuestion:\n{question}\n\nAnswer:"
    input_ids = tokenizer(input_text, return_tensors="pt")

    # ðŸ”¹ Step 6: Generate Response
    with torch.no_grad():
        outputs = model.generate(**input_ids, max_new_tokens=100)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ðŸ”¹ Step 7: Print Response
    print("\nðŸ’¬ AI Response:\n", response, "\n")
